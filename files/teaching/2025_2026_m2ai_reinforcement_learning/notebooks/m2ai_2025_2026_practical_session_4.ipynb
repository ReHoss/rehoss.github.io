{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85018b20",
   "metadata": {},
   "source": [
    "# Practical Session 4: Approximation Methods in Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2fbd8",
   "metadata": {},
   "source": [
    "##### *M2 Artificial Intelligence (Paris Saclay University) - Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5eea5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0fefb",
   "metadata": {},
   "source": [
    "In the case where the state or action spaces are very large (or continuous), it is not possible to represent the value function or the policy as a simple vector (a.k.a. \"tabular\" representation). Instead, the use of **function approximation** techniques to estimate these functions is required.\n",
    "\n",
    "--\n",
    "\n",
    "Good references for the approximation part of Reinforcement Learning are again *J. Kwon - \"An Introduction to Reinforcement Learning: From theory to algorithms, 2024\"* and the classic book by *R. Sutton and A. Barto - \"Reinforcement Learning: An Introduction, 2nd edition, 2018\"*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2c0e1",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654537b",
   "metadata": {},
   "source": [
    "## Part I: Semi-Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed25ec",
   "metadata": {},
   "source": [
    "### Function Approximation\n",
    "\n",
    "Recall that one of the main challenges in Reinforcement Learning is to estimate value functions (state-value $v^\\pi(s)$ or action-value $q^\\pi(s,a)$). In the *function approximation* setting, we assume that the value function can be represented as a parametric function $v(s; \\theta)$ (or $q(s,a; \\theta)$) where $\\theta \\in \\Theta \\subset \\mathbb{R}^d$ is a vector of parameters (*e.g.* neural network weights, linear coefficients, SVM parameters, etc.).\n",
    "\n",
    "An important kind of parametrisation is the **linear function approximation**, where the value function is represented as a linear combination of features:\n",
    "$$v(s; \\theta) = \\phi(s)^T \\theta = \\sum_{i=1}^d \\phi_i(s) \\theta_i$$\n",
    "where $\\phi(s) = [\\phi_1(s), \\phi_2(s), \\ldots, \\phi_d(s)]^T$ is a vector of $d$ features extracted from the state $s$ (ex: polynomial features, neural network etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1cd54",
   "metadata": {},
   "source": [
    "### Solving the Bellman Equation with Function Approximation\n",
    "\n",
    "The Bellman equations are given by:\n",
    "\n",
    "- for the value functions, the operators $\\mathcal{T}_\\gamma^\\pi$ and $\\mathcal{T}_\\gamma^*$ are defined as:\n",
    "$$\n",
    "\\mathcal{T}_\\gamma^\\pi v(s) = r(s, \\pi(s)) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot | s, \\pi(s))}[v(s')]\n",
    "$$\n",
    "$$\n",
    "\\mathcal{T}_\\gamma^* v(s) = \\max_{a \\in A} \\left( r(s, a) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot | s, a)}[v(s')] \\right)\n",
    "$$\n",
    "- for the Q-functions:\n",
    "$$\n",
    "\\mathcal{T}_\\gamma^\\pi q(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot | s, a)}[q(s', \\pi(s'))]\n",
    "$$\n",
    "$$\n",
    "\\mathcal{T}_\\gamma^* q(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot | s, a)}\\left[ \\max_{a' \\in A} q(s', a') \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Consequently, in the function approximation setting, we want to find a parameter vector $\\theta^*$ such that the approximated value function $v(s; \\theta^*)$ (or $q(s,a; \\theta^*)$) satisfies the Bellman equation:\n",
    "\n",
    "- for the value function:\n",
    "$$v(s; \\theta^*) = \\mathcal{T}_\\gamma^\\pi v(s; \\theta^*) \\quad \\text{or} \\quad v(s; \\theta^*) = \\mathcal{T}_\\gamma^* v(s; \\theta^*)$$\n",
    "\n",
    "- for the Q-function:\n",
    "$$q(s,a; \\theta^*) = \\mathcal{T}_\\gamma^\\pi q(s,a; \\theta^*) \\quad \\text{or} \\quad q(s,a; \\theta^*) = \\mathcal{T}_\\gamma^* q(s,a; \\theta^*)$$\n",
    "\n",
    "However, due to the function approximation, it is generally not possible to satisfy these equations exactly for all states (or state-action pairs). Instead, we aim to find a parameter vector $\\theta^*$ that minimizes the **mean squared Bellman error** over the state (or state-action) space.\n",
    "$$\n",
    "\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{s \\sim \\rho(s)} \\left[ \\left( v(s; \\theta) - \\mathcal{T}_\\gamma^\\pi v(s; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{(s,a) \\sim \\rho(s,a)} \\left[ \\left( q(s,a; \\theta) - \\mathcal{T}_\\gamma^\\pi q(s,a; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "where $\\rho(s)$ (or $\\rho(s,a)$) is a distribution over states (or state-action pairs), often chosen as the visitation distribution induced by the policy $\\pi$.\n",
    "<br>\n",
    "\n",
    "--\n",
    "\n",
    "**Theorem (optimality necessary condition)**:\n",
    "The function $v(s; \\theta^*)$ (or $q(s,a; \\theta^*)$) minimises the mean squared Bellman error, i.e. these functions solve the optimisation problems above, if and only if the following condition holds:\n",
    "\n",
    "- for the value function:\n",
    "$$\n",
    " \\left( v(s; \\theta^*) - \\mathcal{T}_\\gamma^\\pi v(s; \\theta^*) \\right) \\nabla_\\theta v(s; \\theta^*) = 0\n",
    "$$\n",
    "\n",
    "or (for the optimal value function):\n",
    "\n",
    "$$\n",
    " \\left( v(s; \\theta^*) - \\mathcal{T}_\\gamma^* v(s; \\theta^*) \\right) \\nabla_\\theta v(s; \\theta^*) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "- for the Q-function:\n",
    "$$\n",
    " \\left( q(s,a; \\theta^*) - \\mathcal{T}_\\gamma^\\pi q(s,a; \\theta^*) \\right) \\nabla_\\theta q(s,a; \\theta^*) = 0\n",
    "$$\n",
    "\n",
    "or (for the optimal Q-function):\n",
    "\n",
    "$$\n",
    " \\left( q(s,a; \\theta^*) - \\mathcal{T}_\\gamma^* q(s,a; \\theta^*) \\right) \\nabla_\\theta q(s,a; \\theta^*) = 0\n",
    "$$\n",
    "\n",
    "--\n",
    "\n",
    "The theorem above is a standard criterion in optimisation theory (critical point condition).\n",
    "\n",
    "Consequently, a necessary condition for a value function $v(s; \\theta^*)$ to minimise the mean squared Bellman error is that its parameters $\\theta^*$ satisfy:\n",
    "$$\n",
    "\\mathbb{E}_{s \\sim \\rho(s)} \\left[ \\left( v(s; \\theta^*) - \\mathcal{T}_\\gamma^\\pi v(s; \\theta^*) \\right) \\nabla_\\theta v(s; \\theta^*) \\right] = 0\n",
    "$$\n",
    "and similarly for the other cases.\n",
    "\n",
    "This suggests a stochastic approximation approach to find such a parameter vector $\\theta^*$.\n",
    "Indeed, stochastic approximation methods aim in particular to find zeros of functions defined as expectations.\n",
    "\n",
    "This condition forms the basis for the **Semi-Gradient Policy Evaluation** algorithm. The term \"semi-gradient\" refers to the fact this gradient method is performed with a moving target (the Bellman operator depends on the current parameter $\\theta_n \\in \\Theta$ at iteration $n$).\n",
    "\n",
    "### Semi-Gradient Policy Evaluation Algorithm\n",
    "The **Semi-Gradient Policy Evaluation** algorithm is defined as follows:\n",
    "- Initialise the parameter vector $\\theta_0 \\in \\Theta$, set $n = 0$.\n",
    "- Repeat until convergence:\n",
    "    - Generate a transition $(S_n, A_n, R_n, S_{n+1})$ by following policy $\\pi$ in the environment.\n",
    "    - Update the parameter vector:\n",
    "    $$\\theta_{n+1} = \\theta_n + \\alpha_n \\left( R_n + \\gamma v(S_{n+1}; \\theta_n) - v(S_n; \\theta_n) \\right) \\nabla_\\theta v(S_n; \\theta_n) = \\theta_n + \\alpha_n (v(S_n; \\theta_n) - \\mathcal{T}_\\gamma^\\pi v(S_n; \\theta_n)) \\nabla_\\theta v(S_n; \\theta_n)$$\n",
    "    \n",
    "    - Increment $n$ by 1.\n",
    "    where $\\{\\alpha_n\\}_{n \\geq 0}$ is a sequence of positive step-sizes satisfying the Robbins-Monro conditions.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75895d59",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Implementation of Semi-Gradient policy evaluation\n",
    "\n",
    "Implement the Semi-Gradient TD(0) algorithm for estimating the state-value function $v^\\pi$ using linear function approximation. Use polynomial features for the state representation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809ffe9",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Implementation of Semi-Gradient Q-learning\n",
    "\n",
    "Give the semi-gradient Q-learning update rule for estimating the optimal state-action value function $q^*$.\n",
    "Implement the algorithm with different types of function approximators (linear, neural networks, etc.).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93fa2c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa6bcf",
   "metadata": {},
   "source": [
    "## Part II: Policy Gradient Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be110bd",
   "metadata": {},
   "source": [
    "### Policy Parametrisation\n",
    "\n",
    "This approach is similar to the value function approximation methods seen previously. Here, the policy is not derived from a value function estimate, but is directly parametrised as a function $\\pi(a|s; \\theta)$ where $\\theta \\in \\Theta \\subset \\mathbb{R}^d$ is a vector of parameters.\n",
    "\n",
    "The policy is considered differentiable w.r.t. $\\theta \\in \\Theta$ and verifies\n",
    "$$\n",
    "\\pi(a|s; \\theta) \\geq 0, \\quad \\sum_{a \\in A} \\pi(a|s; \\theta) = 1, \\quad \\forall s \\in S, a \\in A, \\theta \\in \\Theta\n",
    "$$\n",
    "\n",
    "#### Example: Softmax Policy\n",
    "A common choice for discrete action spaces is the **softmax policy** defined as:\n",
    "$$\\pi(a|s; \\theta) = \\frac{e^{f(s,a; \\theta)}}{\\sum_{a' \\in A} e^{f(s,a'; \\theta)}}$$\n",
    "where $f(s,a; \\theta)$ is a parametric function (ex: linear function, neural network, etc.) that outputs a score for each action $a \\in \\mathcal{A}$ given the state $s \\in \\mathcal{S}$.\n",
    "\n",
    "\n",
    "### Optimising the Policy Parameters\n",
    "\n",
    "The goal of policy gradient methods is to find the optimal parameter vector $\\theta^*$ that maximises the expected cumulative reward:\n",
    "$$\\theta^* = \\arg \\max_{\\theta \\in \\Theta} \\mathbb{E}_{S \\sim \\rho} \\left[ v^{\\pi(\\cdot| \\cdot ; \\theta)}_\\gamma(S) \\right]$$\n",
    "where $\\rho$ is a distribution over states (ex: initial state distribution, stationary distribution induced by the policy, etc.).\n",
    "\n",
    "This optimisation problem can be solved using gradient ascent methods. The following key theorem provides an expression of a stochastic estimator of the gradient of the above objective function, to be used in a stochastic gradient ascent algorithm. Note that there are different versions of the policy gradient theorem.\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "**Theorem (Policy Gradient Theorem):**\n",
    "Suppose that the policy $\\pi(a|s; \\theta)$ is differentiable w.r.t. $\\theta \\in \\Theta$. Let $\\rho$ be the distribution over states used in the objective function.\n",
    "\n",
    "Then, the gradient of the expected cumulative reward w.r.t. $\\theta$ is given by:\n",
    "$$\\nabla_\\theta \\mathbb{E}_{S \\sim \\rho} \\left[ v^{\\pi(\\cdot| \\cdot ; \\theta)}_\\gamma(S) \\right] = \\mathbb{E}_{S \\sim \\rho, \\pi(\\cdot | \\cdot ; \\theta)} \\left[ \\sum_{t = 0}^\\infty \\gamma^t r_t(S_t, A_t) \\sum_{t = 0}^\\infty \\nabla_\\theta \\log \\pi(A_t | S_t; \\theta) \\right]$$\n",
    "\n",
    "\n",
    "--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606f8cb",
   "metadata": {},
   "source": [
    "Remark:\n",
    "\n",
    "The policy gradient theorem says that the gradient of the expected cumulative reward is an expectation. This expectation can be estimated using independent samples of MDP trajectories obtained by interacting with the environment using the current policy $\\pi(\\cdot | \\cdot ; \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533af5f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: On the hypothesis of the Policy Gradient Theorem\n",
    "\n",
    "Why the hypothesis of differentiability and positivity of the policy $\\pi(a|s; \\theta)$ are necessary to derive the Policy Gradient Theorem?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32644824",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Implementation of the Policy Gradient Theorem\n",
    "\n",
    "Implement the policy gradient theorem to learn a policy for a simple MDP (ex: the Stochastic Inventory Problem, a Gymnasium 'classic control' environment, or an environment with continuous state or action space).\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m2ai_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
