{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85018b20",
   "metadata": {},
   "source": [
    "# Practical Session 2: Discounted Infinite-horizon Markov Decision Problems and Iterative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2fbd8",
   "metadata": {},
   "source": [
    "##### *M2 Artificial Intelligence (Paris Saclay University) - Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5eea5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0fefb",
   "metadata": {},
   "source": [
    "First, this notebook introduces infinite-horizon Markov decision processes with the expected total discounted reward criterion.\n",
    "These models are the best understood of all infinite-horizon Markov decision problems.\n",
    "\n",
    "The infinite-horizon discounted reward criterion lead to so-called \"fixed point equations\" for the value function, which are at the heart of the most standard Reinforcement Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377003a6",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152a301",
   "metadata": {},
   "source": [
    "## Part I: Policy Evaluation in Discounted MDPs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df398c",
   "metadata": {},
   "source": [
    "### Definition (Expected Total Discounted Reward):\n",
    "\n",
    "Let $\\pi = (d_0, d_1, \\ldots)$ be a policy.\n",
    "\n",
    "The **expected total discounted reward** with discount factor $\\gamma \\in [0, 1)$ when starting from state $s$ and following policy $\\pi$ is defined as:\n",
    "$$\n",
    "v^\\pi_\\gamma(s) = \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(X_t, Y_t) \\mid X_0 = s \\right] = \\lim_{N \\to \\infty} \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^{N} \\gamma^t r(X_t, Y_t) \\mid X_0 = s \\right]\n",
    "$$\n",
    "\n",
    "Because of the following theorem, *only Markovian policies need to be considered in discounted MDPs.*\n",
    "\n",
    "**Theorem (Reduction to Markovian Policies):**\n",
    "For any policy $\\pi = (d_0, d_1, \\ldots)$ (which may be history-dependent), there exists a Markovian policy $\\pi' = (d'_0, d'_1, \\ldots)$ such that:\n",
    "$$\n",
    "v^\\pi_\\gamma(s) = v^{\\pi'}_\\gamma(s), \\quad \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "Hence, the optimal value function over all policies can be expressed as:\n",
    "$$\n",
    "v^*_\\gamma(s) = \\sup_{\\pi \\in \\Pi} v^\\pi_\\gamma(s) = \\sup_{\\pi \\in \\Pi_{M}} v^\\pi_\\gamma(s), \\quad \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "where $\\Pi_{M}$ is the set of Markovian policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dfe0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Existence of the limit\n",
    "Give a condition on the reward function $r$ that ensures the existence of the limit in the above definition.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e027c9",
   "metadata": {},
   "source": [
    "\n",
    "### Definition (Policy Evaluation Equation):\n",
    "\n",
    "Let $\\pi = (d_0, d_1, \\ldots)$ be a Markovian policy. \n",
    "One has\n",
    "\n",
    "$$\n",
    "v^\\pi_\\gamma(s) = \\sum_{a \\in A} d_0(a|s) r(s, a) + \\gamma \\sum_{a \\in A} d_0(a|s) \\sum_{s' \\in S} P(s'|s, a) \\left[ d_1(a'|s') r(s', a') + \\gamma \\sum_{a' \\in A} d_1(a'|s') \\sum_{s'' \\in S} P(s''|s', a') \\ldots \\right]\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "v^\\pi_\\gamma(s) = \\sum_{a \\in A} d_0(a|s) \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v^{\\pi'}_\\gamma(s') \\right]\n",
    "$$\n",
    "\n",
    "where $\\pi' = (d_1, d_2, \\ldots)$ is the policy obtained by removing the first decision rule of $\\pi$.\n",
    "The above equation shows that the discounted reward corresponsings to policy $\\pi$ equals the discounted reward in a one-period problem in which the decision maker uses decision rule $d_0$ and receives an immediate reward $r(s, a)$ plus the expected discounted reward of policy $\\pi'$ starting from the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece74b2",
   "metadata": {},
   "source": [
    "When the policy $\\pi$ is stationary, i.e., $\\pi = (d, d, \\ldots)$, recall that we denote it by $\\pi = d$.\n",
    "In this case, the above equation simplifies to:\n",
    "$$\n",
    "v^\\pi_\\gamma(s) = \\sum_{a \\in A} \\pi(a|s) \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v^\\pi_\\gamma(s') \\right]\n",
    "$$\n",
    "Here, the policy $\\pi$ is the same in the left-hand side and the right-hand side of the equation.\n",
    "\n",
    "Consequently, when the policy $\\pi$ is stationary, the value function $v^\\pi_\\gamma$ satisfies the following system of linear equations (one equation for each state $s \\in S$):\n",
    "$$\n",
    "v(s) = \\sum_{a \\in A} \\pi(a|s) \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v(s') \\right]\n",
    "\\coloneqq T^\\pi_\\gamma v(s),\n",
    "\\quad \\forall s \\in S\n",
    "$$\n",
    "It is a functional equation (term used by Bellman) where the unknown is the function $v: S \\to \\mathbb{R}$.\n",
    "In functional form, it can be written as:\n",
    "$$\n",
    "v = T^\\pi_\\gamma v\n",
    "$$\n",
    "where $T^\\pi_\\gamma$ is called the **policy evaluation operator** associated with policy $\\pi$ and discount factor $\\gamma$.\n",
    "A element $v$ satisfying this equation is called a **fixed point** of the operator $T^\\pi_\\gamma$.\n",
    "\n",
    "\n",
    "The following theorem ensures that this system has a unique solution.\n",
    "\n",
    "**Theorem (Policy Evaluation)**\n",
    "For any stationary policy $\\pi = d$ and any discount factor $\\gamma \\in [0, 1)$, the system of equations\n",
    "$$\n",
    "v(s) = \\sum_{a \\in A} \\pi(a|s) \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v(s') \\right], \\quad \\forall s \\in S\n",
    "$$\n",
    "has a unique solution $v^\\pi_\\gamma$, which is the expected total discounted reward corresponding to policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da222dd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Policy Evaluation for deterministic Markovian policies\n",
    "\n",
    "Write the system of linear equations satisfied by the value function $v^\\pi_\\gamma$ for any deterministic Markovian policy $\\pi$ in a general MDP.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684c32f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Policy Evaluation of the Two-State MDP\n",
    "Evaluate the stationary Markov deterministic policy $\\pi$ defined by $\\pi(s_1) = a_{1, 1}$ and $\\pi(s_2) = a_{2, 1}$ in the two-state MDP introduced in Practical Session 1, for a fixed discount factor $\\gamma \\in [0, 1)$.  \n",
    "Evaluate the stationary Markov deterministic policy $\\beta$ defined by $\\beta(s_1) = a_{1, 2}$ and $\\beta(s_2) = a_{2, 1}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309db26f",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b9e37",
   "metadata": {},
   "source": [
    "## Part II: Optimality Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c351b",
   "metadata": {},
   "source": [
    "### Definition (Optimality Equations):\n",
    "\n",
    "Suppose that the action spaces $A(s)$ are finite for all states $s \\in S$ (otherwise, max operators would need to be replaced by sup operators and theorems would need additional assumptions).\n",
    "Recall that in finite-horizon MDPs, the optimal value functions satisfy the **optimality equations** (finite-horizon Bellman equations):\n",
    "$$\n",
    "v_n(s) = \\max_{a \\in A} \\left[ r(s, a) + \\sum_{s' \\in S} P(s'|s, a) v_{n+1}(s') \\right], \\quad \\forall s \\in S\n",
    "$$\n",
    "\n",
    "Passing to the limit suggests that in infinite-horizon discounted MDPs, the optimal value function $v^*_\\gamma$ satisfies the following system of equations (one equation for each state $s \\in S$):\n",
    "$$\n",
    "v(s) = \\max_{a \\in A} \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v(s') \\right]\n",
    "\\coloneqq T^*_\\gamma v(s), \\quad \\forall s \\in S\n",
    "$$\n",
    "It is a functional equation where the unknown is the function $v: S \\to \\mathbb{R}$.\n",
    "In functional form, it can be written as:\n",
    "$$\n",
    "v = T^*_\\gamma v\n",
    "$$\n",
    "where $T^*_\\gamma$ is called the **optimality operator** associated with discount factor $\\gamma$ (note the presence of the maximum operator in this case).\n",
    "\n",
    "\n",
    "**Theorem (Existence and Uniqueness of the Solution to the Optimality Equations)**\n",
    "For any discount factor $\\gamma \\in [0, 1)$, the system of equations\n",
    "$$\n",
    "v(s) = \\max_{a \\in A} \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v(s') \\right], \\quad \\forall s \\in S\n",
    "$$\n",
    "has a unique solution $v^*_\\gamma$, which is the optimal value function in the infinite-horizon discounted MDP.\n",
    "\n",
    "\n",
    "\n",
    "**Theorem (Existence of Optimal Stationary Deterministic Policies)**\n",
    "For any discount factor $\\gamma \\in [0, 1)$, there exists a stationary deterministic Markov policy $\\pi^* = d^*$ that is optimal, i.e.,\n",
    "$$\n",
    "v^{\\pi^*}_\\gamma(s) = v^*_\\gamma(s), \\quad \\forall s \\in S\n",
    "$$\n",
    "Moreover, any stationary deterministic Markov policy $\\pi^* = d^*$ satisfying\n",
    "$$\n",
    "d^*(s) \\in \\arg\\max_{a \\in A} \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v^*_\\gamma(s') \\right], \\quad \\forall s \\in S\n",
    "$$\n",
    "is optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1255fb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Optimal Policy for the Two-State MDP\n",
    "\n",
    "Find the optimal value function $v^*_\\gamma$ and an optimal stationary deterministic policy $\\pi^* = d^*$ for the two-state MDP introduced in Practical Session 1, for a fixed discount factor $\\gamma \\in \\{0.9\\}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f799bab",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be62ff7",
   "metadata": {},
   "source": [
    "## Part III: Value and Policy Iteration Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cb38a",
   "metadata": {},
   "source": [
    "### Definition (Value Iteration Algorithm):\n",
    "\n",
    "The **Value Iteration Algorithm** is a fundamental iterative method to compute the optimal value function $v^*_\\gamma$ in infinite-horizon discounted MDPs. Basically, it consists in applying iteratively the optimality operator $T^*_\\gamma$ (this scheme is called fixed-point iteration) starting from an initial value function (often chosen as the zero function).\n",
    "\n",
    "It is defined as follows:\n",
    "- Initialize $v_0(s) = 0$ for all $s \\in S$, choose $\\epsilon > 0$ and set $n = 0$.\n",
    "- Repeat until convergence (i.e., until $\\|v_{n+1} - v_n\\|_\\infty < \\epsilon$):\n",
    "    $$\n",
    "    v_{n+1}(s) = \\max_{a \\in A} \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v_n(s') \\right], \\quad \\forall s \\in S\n",
    "    $$\n",
    "    - Increment $n$ by 1.\n",
    "\n",
    "\n",
    "**Informal Theorem (Convergence of the Value Iteration Algorithm)**\n",
    "The Value Iteration Algorithm can approximate the optimal value function $v^*_\\gamma$ with arbitrary precision that depends on the choice of $\\epsilon$. The convergence does not depend on the topology of the state and action spaces.\n",
    "The policy obtained by acting greedily with respect to the approximate value function can be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a0fed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Value Iteration for the Inventory Control Problem\n",
    "\n",
    "Implement the Value Iteration Algorithm to compute the optimal value function $v^*_\\gamma$ and an optimal stationary deterministic policy $\\pi^* = d^*$ for an infinite-horizon stochastic inventory control problem (or a problem of your choice) introduced in Practical Session 1, with $\\gamma \\in [0, 1)$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65186345",
   "metadata": {},
   "source": [
    "### Definition (Policy Iteration Algorithm):\n",
    "\n",
    "The **Policy Iteration Algorithm** is an iterative method to compute the optimal value function $v^*_\\gamma$ and an optimal policy $\\pi^* = d^*$ in infinite-horizon discounted MDPs.\n",
    "\n",
    "It is defined as follows:\n",
    "- Initialize a stationary deterministic Markovian policy $\\pi_0 = d_0$, set $n = 0$.\n",
    "- Repeat until convergence (i.e., until $\\pi_{n+1} = \\pi_n$):\n",
    "    - **Policy Evaluation Step**: Compute the value function $v^{\\pi_n}_\\gamma$ corresponding to policy $\\pi_n$ by solving the system of linear equations:\n",
    "    $$\n",
    "    v(s) = \\sum_{a \\in A} \\pi_n(a|s) \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v(s') \\right], \\quad \\forall s \\in S\n",
    "    $$\n",
    "    - **Policy Improvement Step**: Update the policy by setting, for all states $s \\in S$,\n",
    "    $$\n",
    "    \\pi_{n+1}(s) \\in \\arg\\max_{a \\in A} \\left[ r(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) v^{\\pi_n}_\\gamma(s') \\right]\n",
    "    $$\n",
    "    - Increment $n$ by 1.\n",
    "\n",
    "Note that the following theorem holds on the convergence of the Policy Iteration Algorithm.\n",
    "\n",
    "**Theorem (Convergence of the Policy Iteration Algorithm)**\n",
    "The Policy Iteration Algorithm converges in a finite number of iterations to an optimal stationary deterministic Markovian policy $\\pi^* = d^*$ and its corresponding optimal value function $v^*_\\gamma$ when the action spaces $A(s)$ are finite for all states $s \\in S$ and the state space $S$ is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec781d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Policy Iteration for the Inventory Control Problem\n",
    "\n",
    "Implement the Policy Iteration Algorithm to compute the optimal value function $v^*_\\gamma$ and an optimal stationary deterministic policy $\\pi^* = d^*$ for the infinite-horizon stochastic inventory control problem (or a problem of your choice) based on the problem introduced in Practical Session 1, where $\\gamma \\in [0, 1)$.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m2ai_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
