{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a37a6c9860e28d",
   "metadata": {},
   "source": [
    "# Practical Session 1: Markov Decision Processes and Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c08d0b",
   "metadata": {},
   "source": [
    "##### *M2 Artificial Intelligence (Paris Saclay University) - Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68d5df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9bb174ec7f892",
   "metadata": {},
   "source": [
    "From Bellman's preface to \"Dynamic Programming\" (1957):\n",
    "\n",
    "*The purpose of this work is to provide an introduction to the mathematical theory of multi-stage decision processes. Since these constitute a somewhat formidable set of terms we have coined the term “dynamic programming” to describe the subject matter.*\n",
    "\n",
    "*The problems we treat are programming problems, to use a terminology now popular. The adjective “dynamic,” however, indicates that we are interested in processes in which time plays a significant role, and in which the order of operations may be crucial. However, an essential feature of our approach will be the reinterpretation of many static processes as dynamic processes in which time can be artificially introduced.*\n",
    "\n",
    "The term \"programming\" here does not refer to computer programming, but rather to the process of planning or decision-making. See this [Wikipedia section](https://en.wikipedia.org/wiki/Dynamic_programming#History_of_the_name).\n",
    "\n",
    "This practical session is based on the chapter 4 of the book \"Markov Decision Processes: Discrete Stochastic Dynamic Programming\" by Martin L. Puterman (1994)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3d3dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f298967bcca51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:48.944241Z",
     "start_time": "2025-11-09T19:16:48.939693Z"
    }
   },
   "source": [
    "## Part I: Modelling a Decision Problem as a Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d1c4a45bd6c9c",
   "metadata": {},
   "source": [
    "The most fundamental sequential decision-making model is the **Markov Decision Process (MDP)**.\n",
    "\n",
    "### Definition (Markov Decision Process):\n",
    "\n",
    "A Markov Decision Process provides a formal way to describe an environment for decision-making under uncertainty. A (finite) MDP is defined by a 4-tuple $(\\mathcal{S}, \\mathcal{A}, P, r)$:\n",
    "\n",
    "* **$\\mathcal{S}$**: A finite set of **states** $s \\in \\mathcal{S}$.\n",
    "* **$\\mathcal{A}$**: A finite set of **actions** $a \\in \\mathcal{A}$. We often write $\\mathcal{A}(s)$ to denote the set of actions available in state $s$.\n",
    "* **$P$**: The **transition probability function**, $P(s' | s, a) = \\mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]$. This defines the \"dynamics\" of the environment.\n",
    "* **$r$**: The **reward function**, $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. This gives the immediate reward $r(s, a)$ received after taking action $a$ in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c05b7",
   "metadata": {},
   "source": [
    "### Example: The Two-State Problem\n",
    "\n",
    "The system can be in one of two states, $s_1$ or $s_2$.\n",
    "\n",
    "* In state **$s_1$**, the decision-maker can choose $a_{1,1}$ or $a_{1,2}$.\n",
    "* In state **$s_2$**, only action $a_{2,1}$ is available.\n",
    "\n",
    "The problem dynamics and rewards are summarized in the following table:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\text{State} & \\text{Action} & \\text{Next State(s)} & \\text{Reward} \\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\large s_1 & a_{1,1} & \\begin{cases} s_1 & (p=0.5) \\\\ s_2 & (p=0.5) \\end{cases} & 5 \\\\\n",
    "\\hline\n",
    "\\large s_1 & a_{1,2} & s_2 \\quad (p=1.0) & 10 \\\\\n",
    "\\hline\n",
    "\\large s_2 & a_{2,1} & s_2 \\quad (p=1.0) & -1 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b885a480635b105",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Define the MDP for the Two-State Problem\n",
    "\n",
    "Define the components of the MDP for the two-state problem:\n",
    "\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):**\n",
    "\n",
    "* **Action Space ($\\mathcal{A}(s)$):**\n",
    "\n",
    "* **Reward Function ($r(s, a)$):**\n",
    "\n",
    "* **Transition Probabilities ($P(s' | s, a)$):**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df424d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):** $\\{s_1, s_2\\}$\n",
    "\n",
    "* **Action Space ($\\mathcal{A}(s)$):**  \n",
    "  * $\\mathcal{A}(s_1) = \\{a_{1,1}, a_{1,2}\\}$\n",
    "  * $\\mathcal{A}(s_2) = \\{a_{2,1}\\}$\n",
    "\n",
    "* **Reward Function ($r(s, a)$):**\n",
    "  * $r(s_1, a_{1,1}) = 5$\n",
    "  * $r(s_1, a_{1,2}) = 10$\n",
    "  * $r(s_2, a_{2,1}) = -1$\n",
    "\n",
    "* **Transition Probabilities ($P(s' | s, a)$):**\n",
    "    * $P(s_1 | s_1, a_{1,1}) = 0.5$, $P(s_2 | s_1, a_{1,1}) = 0.5$\n",
    "    * $P(s_1 | s_1, a_{1,2}) = 0$, $P(s_2 | s_1, a_{1,2}) = 1$\n",
    "    * $P(s_1 | s_2, a_{2,1}) = 0$, $P(s_2 | s_2, a_{2,1}) = 1$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d617c",
   "metadata": {},
   "source": [
    "### Example: Single Product Stochastic Inventory Control\n",
    "\n",
    "\n",
    "#### Description:\n",
    "Inventory models were among the first problems solved using Markov decision problem methods, and their study has motivated many theoretical developments.\n",
    "\n",
    "Each month, the manager of a warehouse determines current inventory (stock on hand) of a single product. Based on this information, he decides whether or not to order additional stock from a supplier.\n",
    "In doing so, he is faced with a tradeoff between the costs associated with keeping inventory and the lost sales or penalties associated with being unable to satisfy customer demand for the product.\n",
    "The manager's objective is to maximize some measure of profit (sales revenue less inventory holding and ordering costs) over the decision-making horizon.\n",
    "Demand for the product is random with a known probability distribution.\n",
    "\n",
    "We formulate a model under the following set of simplifying assumptions. Generalizations which make the model more realistic are explored through problems at the end of the chapter.\n",
    "1. The decision to order additional stock is made at the beginning of each month\n",
    "and delivery occurs instantaneously.\n",
    "2. Demand for the product arrives throughout the month but all orders are filled\n",
    "on the last day of the month.\n",
    "3. If demand exceeds inventory, the customer goes elsewhere to purchase\n",
    "product; that is, there is no backlogging of unfilled orders so that excess\n",
    "demand is lost.\n",
    "4. The revenues, costs, and the demand distribution do not vary from month to\n",
    "month.\n",
    "5. The product is sold only in whole units.\n",
    "6. The warchouse has capacity of $M$ units.\n",
    "\n",
    "Let $s_t$ denote the inventory on hand at the beginning of month $t$, $a_t$ the number of units ordered by the inventory manager in month $t$ and $D_t$ the random demand in month $t$. We assume that the demand has a known time-homogeneous probability distribution $p_j = P(D_t = j)$, $j \\in \\mathbb{N}$.\n",
    "The inventory at decision epoch $t + 1$, $s_{t+1}$, is related to the inventory at decision epoch $t$, $s_t$, through the system equation\n",
    "$$ s_{t+1} = \\max\\{s_t + a_t - D_t, 0\\} = [s_t + a_t - D_t]^{+}. $$\n",
    "Because backlogging is not permitted, the inventory level cannot be negative. Thus whenever $s_t + a_t - D_t < 0$, the inventory level at the subsequent decision epoch is 0.\n",
    "We now describe the economic parameters of this model. We express them as values at the start of the month so that we are implicitly considering the time value of money when defining these quantities. We refer to them as present values to\n",
    "\n",
    "A Markov decision process formulation follows.\n",
    "\n",
    "\n",
    "**Expected rewards (expected revenue less ordering and holding costs):**\n",
    "The reward is composed of three parts: the expected revenue from sales, the cost of ordering new stock, and the cost of holding inventory.\n",
    "\n",
    "The one-step expected reward is:\n",
    "$r_t(s, a) = F(s+a) - O(a) - h(s+a), \\quad t=1, 2, \\dots, N-1$\n",
    "\n",
    "\n",
    "- The expected revenue $F(s)$ when having $s$ units of inventory.\n",
    "- $O(a)$ is the cost of ordering $a$ units.\n",
    "- $h(s+a)$ is the cost of holding $s+a$ units of inventory for a month.\n",
    "\n",
    "\n",
    "At the end of the horizon, there is a terminal reward:\n",
    "$r_N(s) = g(s), \\quad t=N$.\n",
    "where $g(s)$ is the value of the terminal inventory.\n",
    "\n",
    "**Transition probabilities:**\n",
    "If the inventory on hand at the beginning of period $t$ is $s$ units and an order is placed for $a$ units, the inventory prior to external demand is $s+a$ units.\n",
    "An inventory level of $s'>0$ at the start of period $t+1$ requires a demand of $s+a-s'$ units in period $t$. This occurs with probability $p_{s+a-s'}$.\n",
    "Because backlogging is not permitted, if the demand in period $t$ exceeds $s+a$ units, then the inventory at the start of period $t+1$ is 0 units.\n",
    "This occurs with probability $\\sum_{k=s+a}^{\\infty} p_k = P(D_t \\ge s+a)$.\n",
    "The probability that the inventory level exceeds $s+a$ units is 0, since demand is non-negative.\n",
    "Assumption 6 constrains the inventory always to be less than or equal to $M$.\n",
    "\n",
    "Consequently,\n",
    "$P(s'|s, a) = \\begin{cases} 0 & \\text{if } M \\ge s' > s+a \\\\ p_{s+a-s'} & \\text{if } M \\ge s+a \\ge s' > 0 \\\\ \\sum_{k=s+a}^{\\infty} p_k & \\text{if } M \\ge s+a \\text{ and } s'=0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b609f391f5b610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Define the MDP for the Inventory Control Problem\n",
    "\n",
    "Define the missing components of the MDP for the inventory control problem:\n",
    "\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):**\n",
    "\n",
    "* **Action Space ($\\mathcal{A}(s)$):**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24673c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):** The state space consists of all possible inventory levels at the beginning of each month. If the warehouse has a maximum capacity of $M$ units, then $\\mathcal{S} = \\{0, 1, 2, \\ldots, M\\}$.\n",
    "* **Action Space ($\\mathcal{A}(s)$):** The action space consists of all possible order quantities that can be placed given the current inventory level $s$. If the maximum order quantity is limited by the warehouse capacity, then $\\mathcal{A}(s) = \\{0, 1, 2, \\ldots, M - s\\}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc5bed",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d01bd",
   "metadata": {},
   "source": [
    "## Part II: Policy and Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba60171",
   "metadata": {},
   "source": [
    "### Definition (Decision Rule and Policy):\n",
    "\n",
    "A **decision rule** $d_t$ at time $t \\in \\{0, 1, \\dots, N-1\\}$ is a function that maps a history of states and actions $H_t$ to a probability distribution over the set of actions $\\mathcal{A}$.\n",
    "$d_t: H_t \\to \\mathcal{P}(\\mathcal{A})$\n",
    "where $H_t = \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times \\dots \\times \\mathcal{A} \\times \\mathcal{S}$ is the set of all possible histories up to time $t$.\n",
    "When the decision rule is deterministic, it maps histories directly to actions:\n",
    "$d_t: H_t \\to \\mathcal{A}$.\n",
    "\n",
    "\n",
    "A **policy** $\\pi$ is a sequence of decision rules for each time step.\n",
    "$\\pi = (d_0, d_1, \\dots, d_{N-1})$\n",
    "\n",
    "A policy is **Markovian** if the decision rules only depend on the current state $s_t$, not the entire history.\n",
    "A policy is **stationary** if the decision rule is the same for all time steps (i.e., $d_t = d$ for all $t$, then $\\pi$ can be represented by a single decision rule $d$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c3300",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Defining Policies\n",
    "\n",
    "Suppose N = 2.\n",
    "\n",
    "Define a deterministic Markovian policy $\\pi = (d_0, d_1)$ and a stochastic Markovian policy $\\pi = (d_0, d_1)$ for the two-state problem defined earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d785d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "There are multiple possible policies. Here are examples of each type:\n",
    "\n",
    "* **Deterministic Markovian Policy:**\n",
    "  * $d_0(s_1) = a_{1,1}$; $\\,\\,\\,$ $d_0(s_2) = a_{2,1}$\n",
    "  * $d_1(s_1) = a_{1,2}$; $\\,\\,\\,$ $d_1(s_2) = a_{2,1}$\n",
    "\n",
    "* **Stochastic Markovian Policy:**\n",
    "  * $d_0(s_1) = [0.7 \\text{ for } a_{1,1}, 0.3 \\text{ for } a_{1,2}]$; $\\,\\,\\,$ $d_0(s_2) = [1.0 \\text{ for } a_{2,1}]$\n",
    "  * $d_1(s_1) = [0.4 \\text{ for } a_{1,1}, 0.6 \\text{ for } a_{1,2}]$; $\\,\\,\\,$ $d_1(s_2) = [1.0 \\text{ for } a_{2,1}]$\n",
    "\n",
    "  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc61282",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: On the types of Policies\n",
    "\n",
    "In the two-state problem defined earlier, what can be said about the set of history-dependent policies compared to the set of Markovian policies?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b347971",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "    \n",
    "If the system is in state $s_1$ at any time step, then the history must have been $(s_1, a_{1,1}, s_1, a_{1,1}, \\dots, s_1, a_{1,1})$ so that knowing the history would be redundant to the decision maker: only the length of the history matters, not its specific content. It is possible to make a decision based on the length of the history alone.\n",
    "\n",
    "In state $s_2$, the situation is different. Many possible histories could result in the process being in state $s_2$, but in $s_2$, only action $a_{2,1}$ is available. Thus, regardless of the history, the decision-maker has no choice but to select $a_{2,1}$. Therefore, the decision rule in state $s_2$ is  independent of the history.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b9b0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Cardinality of Deterministic, Markovian Policies\n",
    "What is the size of the set of deterministic, non-stationary Markovian policies for an MDP with $|\\mathcal{S}|$ states and $|\\mathcal{A}|$ actions?\n",
    "\n",
    "What is the size of the set of deterministic, stationary Markovian policies for the same MDP?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede958a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "The size of the set of all possible deterministic, non-stationary Markovian policies for an MDP with $|\\mathcal{S}|$ states and $|\\mathcal{A}|$ actions is given by $|\\mathcal{A}|^{|\\mathcal{S}| \\times N}$, where $N$ is the number of time steps in the decision-making horizon. This is because for each state at each time step, there are $|\\mathcal{A}|$ choices of actions, leading to a total of $|\\mathcal{A}|^{|\\mathcal{S}| \\times N}$ unique policies.\n",
    "\n",
    "For deterministic, stationary Markovian policies, the size of the set is $|\\mathcal{A}|^{|\\mathcal{S}|}$, which is the size of the set of all possible mappings from states to actions, since the same action mapping is used at every time step.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05401d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: \n",
    "\n",
    "Modify the previous model by adding an additional action $a_{1,3}$ in state $s_1$ that leads back to state $s_1$ with probability 1, *i.e.* $P(s_1 | s_1, a_{1,3}) = 1$ with reward $r(s_1, a_{1,3}) = 0$. Note that $\\mathcal{A}(s_1) = \\{a_{1,1}, a_{1,2}, a_{1,3}\\}$ now.\n",
    "\n",
    "Give an history-dependent deterministic policy.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5777c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "An example of a history-dependent deterministic policy $\\pi = (d_0, d_1)$ could be:\n",
    "\n",
    "At time step 0:\n",
    "* $d_0(s_1) = a_{1,1}$; $\\,\\,\\,$ $d_0(s_2) = a_{2,1}$\n",
    "\n",
    "At time step 1:\n",
    "\n",
    "|$(s, a)$ | $d_1(s, a, s_1)$  | $d_1(s, a, s_2)$ |\n",
    "|---------|------------------ |------------------|\n",
    "| $(s_1, a_{1,1})$ | $a_{1,2}$  | $a_{2,1}$ |\n",
    "| $(s_1, a_{1,2})$ | x          | $a_{2,1}$ |\n",
    "| $(s_1, a_{1,3})$ | $a_{1,1}$  | x         |\n",
    "| $(s_2, a_{2,1})$ | x          | $a_{2,1}$ |\n",
    "\n",
    "where \"x\" indicates that the history leading to that state-action pair is not possible.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819edf0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Comparing Policies\n",
    "\n",
    "Let $N=2$ and let $R_t \\coloneqq r(S_t, A_t)$ be the reward received at time $t$.\n",
    "\n",
    "Suppose there are two policies $\\pi$ and $\\pi'$ such that:\n",
    "\n",
    "$P^{\\pi}[R_0 = 0, R_1 = 0] = P^{\\pi}[R_0 = 0, R_1 = 1] = P^{\\pi}[R_0 = 1, R_1 = 0] = P^{\\pi}[R_0 = 1, R_1 = 1] = \\frac{1}{4}$\n",
    "\n",
    "and\n",
    "\n",
    "$P^{\\pi'}[R_0 = 0, R_1 = 0] = P^{\\pi'}[R_0 = 1, R_1 = 1] = \\frac{1}{2}$\n",
    "\n",
    "Compare these policies on the basis of their expected total rewards: \n",
    "\n",
    "$E^\\pi[R_1 + R_2] = E^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)]$.\n",
    "\n",
    "Compare the variance of the total rewards under each policy:\n",
    "\n",
    "$Var^\\pi[R_1 + R_2] = Var^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)] = E^\\pi [(\\sum_{t=0}^{N-1} r(S_t, A_t) - E^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)])^2]$\n",
    "\n",
    "Which policy would you prefer and why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617c392",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "For policy $\\pi$:\n",
    "$E^{\\pi}[R_0 + R_1] = \\frac{1}{4}(0+0) + \\frac{1}{4}(0+1) + \\frac{1}{4}(1+0) + \\frac{1}{4}(1+1) = \\frac{1}{4}(0 + 1 + 1 + 2) = \\frac{4}{4} = 1$\n",
    "\n",
    "For policy $\\pi'$:\n",
    "$E^{\\pi'}[R_0 + R_1] = \\frac{1}{2}(0+0) + \\frac{1}{2}(1+1) = \\frac{1}{2}(0 + 2) = \\frac{2}{2} = 1$\n",
    "\n",
    "\n",
    "For the variance under policy $\\pi$: $Var^{\\pi}[R_0 + R_1] = E^{\\pi}[(R_0 + R_1 - 1)^2] = \\frac{1}{4}(0-1)^2 + \\frac{1}{4}(1-1)^2 + \\frac{1}{4}(1-1)^2 + \\frac{1}{4}(2-1)^2 = \\frac{1}{4}(1 + 0 + 0 + 1) = \\frac{2}{4} = 0.5$\n",
    "\n",
    "For the variance under policy $\\pi'$: $Var^{\\pi'}[R_0 + R_1] = E^{\\pi'}[(R_0 + R_1 - 1)^2] = \\frac{1}{2}(0-1)^2 + \\frac{1}{2}(2-1)^2 = \\frac{1}{2}(1 + 1) = \\frac{2}{2} = 1$\n",
    "\n",
    "<br>\n",
    "\n",
    "Both policies yield the same expected total reward of 1. However, policy $\\pi$ has a lower variance (0.5) compared to policy $\\pi'$ (1). A lower variance indicates could be preferable as it suggests more consistent outcomes. In terms of risk, policy $\\pi$ is less risky than policy $\\pi'$.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86613bc922d5e43",
   "metadata": {},
   "source": [
    "### Definition (The Expected Total Reward Criterion):\n",
    "Let $v^\\pi_{N}(s)$ represent the expected total reward over the decision making horizon if policy $\\pi$ is followed, starting from state $s$ at the first decision epoch (time $t=0$):\n",
    "\n",
    "$v^\\pi_{N}(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{N-1} r(S_t, A_t) + r_N(S_N) \\mid S_0 = s \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ece05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Expected Reward for a Markovian Policy\n",
    "Suppose $\\pi$ is a deterministic, Markovian policy. Simplify the expression for the expected total reward $v^\\pi_N(s)$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c35c93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "For a deterministic, Markovian policy $\\pi$, the action taken at each time step $t$ depends only on the current state $s_t$. Therefore, we can express the expected total reward as follows:\n",
    "\n",
    "$v^\\pi_N(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{N-1} r(S_t, \\pi(S_t)) + r_N(S_N) \\mid S_0 = s \\right]$\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9f7ac15df814c",
   "metadata": {},
   "source": [
    "### Finite-Horizon Policy Evaluation\n",
    "\n",
    "Markov decision problem theory and computation is based on using **backward induction** (dynamic programming) to recursively evaluate expected rewards. This section introduces the fundamental recursion of dynamic programming by providing an efficient method to evaluate the expected total reward of a fixed policy.\n",
    "\n",
    "Let $\\pi = (d_1, d_2, \\dots, d_{N-1})$ be a policy. Let $u_t^\\pi: H_t \\to \\mathbb{R}$ denote the total expected reward obtained by using policy $\\pi$ at decision epochs $t, t+1, \\dots, N-1$. If the history at decision epoch $t$ is $h_t \\in H_t$, then for $t < N$:\n",
    "\n",
    "$u_t^\\pi(h_t) = E^\\pi [ \\sum_{n=t}^{N-1} r_n(X_n, Y_n) + r_N(X_N)  | H_t = h_t ]$\n",
    "\n",
    "We can compute this value by an inductive evaluation. To simplify the notation, we assume that we are using a deterministic policy $\\pi$.\n",
    "\n",
    "#### The Finite Horizon-Policy Evaluation Algorithm (for a fixed deterministic policy $\\pi$)\n",
    "\n",
    "1.  **Set** $t=N$ and initialize the terminal reward for all possible final histories $h_N = (h_{N-1}, a_{N-1}, s_N)$:\n",
    "    $u_N^\\pi(h_N) = r_N(s_N)$\n",
    "\n",
    "2.  **If** $t=0$, stop. Otherwise, go to step 3.\n",
    "\n",
    "3.  **Substitute** $t-1$ for $t$ and compute $u_t^\\pi(h_t)$ for each history $h_t = (h_{t-1}, a_{t-1}, s_t)$ using the recursive formula:\n",
    "    $u_t^\\pi(h_t) = r_t(s_t, d_t(h_t)) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, d_t(h_t)) u_{t+1}^\\pi(h_t, d_t(h_t), s')$\n",
    "\n",
    "4.  **Return** to step 2.\n",
    "\n",
    "This inductive scheme reduces the problem of computing expected total rewards over $N + 1$ periods to a sequence of $N$ similar one-period calculations. The expected value of the policy $\\pi$ over the periods from $t$ to $N$ given the history at epoch $t$ is the sum of the immediate reward from taking action $d_t(h_t)$ and the expected total reward over the remaining periods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c04cd5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Computational Complexity of Policy Evaluation\n",
    "\n",
    "How many multiplications are required to compute $v^\\pi_N(s)$ for all $s \\in \\mathcal{S}$ using the finite-horizon policy evaluation algorithm? Express your answer in terms of $N$ (the horizon length), $|\\mathcal{S}|$ (the number of states), and $|\\mathcal{A}|$ (the number of actions).\n",
    "Give also the computational complexity when the policy $\\pi$ is Markovian.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c8341",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Let $K = |\\mathcal{S}|$ be the number of states and $L = |\\mathcal{A}|$ be the number of actions available in each state.\n",
    "\n",
    "For a **non-randomized, history-dependent policy**, the number of possible histories at decision epoch $t$ is $K^{t+1}L^t$. The evaluation for each history at a given step involves a summation over all possible next states, which requires $K$ multiplications.\n",
    "\n",
    "The total number of multiplications required for the evaluation of a history-dependent policy over a horizon of $N$ steps is given by:\n",
    "$$ K^2 \\sum_{i=0}^{N} (KL)^i = K^2 \\frac{(KL)^{N + 1} - 1}{KL - 1} $$\n",
    "\n",
    "For **Markov policies**, the decision depends only on the current state, not the entire history. This significantly reduces the computational complexity. The number of multiplications required decreases to:\n",
    "$$ NK^2 $$\n",
    "This is because for each of the $N$ decision steps (from $N-1$ down to 0), we compute the value for each of the $K$ states, and each computation involves a sum over $K$ next states.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf357f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Policy Evaluation Expressions\n",
    "\n",
    "1. Give the expression for $u_t^\\pi(h_t)$ when the policy $\\pi$ is Markovian.\n",
    "2. Give the expression for $u_t^\\pi(h_t)$ in terms of conditional expectations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055bdc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "##### Answer:\n",
    "1. When the policy $\\pi$ is Markovian, the decision rule $d_t$ depends only on the current state $s_t$. Therefore, we can express $u_t^\\pi(h_t)$ as:\n",
    "\n",
    "$u_t^\\pi(s_t) = r_t(s_t, d_t(s_t)) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, d_t(s_t)) u_{t+1}^\\pi(s')$.\n",
    "\n",
    "2. In terms of conditional expectations, we can express $u_t^\\pi(h_t)$ as:\n",
    "\n",
    "$u_t^\\pi(h_t) = r_t(s_t, d_t(h_t)) + \\mathbb{E} \\left[ u_{t+1}^\\pi(h_t, d_t(h_t), S_{t+1}) \\mid S_t = s_t, A_t = d_t(h_t) \\right]$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953af096",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51836b",
   "metadata": {},
   "source": [
    "## Part III: Optimality Equations and Bellman's Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5fb2a",
   "metadata": {},
   "source": [
    "### Definition (Optimal Policy):\n",
    "An **optimal policy** $\\pi^*$ is a policy that maximizes the expected total reward for every starting state $s$.\n",
    "$v^{\\pi^*}_N(s) \\ge v^\\pi_N(s), \\quad \\text{for all } s \\in \\mathcal{S} \\text{ and for all } \\pi$\n",
    "\n",
    "We seek to characterize the value of the Markov decision problem, $v^*_N(s)$, defined by\n",
    "$v^*_N(s) = \\sup_{\\pi} v^\\pi_N(s), \\quad s \\in \\mathcal{S}$\n",
    "and when the supremum is attained, by\n",
    "$v^*_N(s) = \\max_{\\pi} v^\\pi_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "The expected total reward of an optimal policy $\\pi^*$ satisfies\n",
    "$v^{\\pi^*}_N(s) = v^*_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68215419a6d28709",
   "metadata": {},
   "source": [
    "### Optimality Equations (Bellman Equations) and The Principle of Optimality\n",
    "\n",
    "Let $u_t: H_t \\to \\mathbb{R}$ for $t = 0, 1, \\dots, N$ be a collection of functions defined on the set of histories at each decision epoch.\n",
    "The *optimality equations* for the collection of functions $(u_t)_{t=0}^N$ are given for any $t = N-1, N-2, \\dots, 0$ and for all histories $h_t \\in H_t$ by:\n",
    "\n",
    "- $u_t(h_t) = \\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}(h_t, a, s') \\right\\}$\n",
    "\n",
    "with the terminal condition:\n",
    "\n",
    "- $u_N(h_N) = r_N(s_N)$\n",
    "\n",
    "--\n",
    "\n",
    "**Theorem (Principle of Optimality):**\n",
    "Suppose that the collection of functions $(u_t^*)_{t=0}^N$ satisfies the optimality equations.\n",
    "Then, for each $t = 0, 1, \\dots, N-1$ and for all histories $h_t \\in H_t$, suppose that a policy\n",
    "\n",
    "- $u_t^*(h_t) = \\max_{\\pi} u_t^\\pi(h_t) = \\max_{\\pi} E^\\pi \\left[ \\sum_{n=t}^{N-1} r_n(X_n, Y_n) + r_N(X_N) | H_t = h_t \\right]$\n",
    "\n",
    "- $u_0^*(s) = v^*_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "Moreover, a policy $\\pi^* = (d_0^*, d_1^*, \\dots, d_{N-1}^*)$ defined by\n",
    "\n",
    "- $d_t^*(h_t) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}^*(h_t, a, s') \\right\\}$\n",
    "\n",
    "is an optimal policy, i.e., $v^{\\pi^*}_N(s) \\geq v^\\pi_N(s)$ for all $s \\in \\mathcal{S}$ and for all policies $\\pi$ (and $v^{\\pi^*}_N(s) = v^*_N(s)$ for all $s \\in \\mathcal{S}$).\n",
    "\n",
    "--\n",
    "\n",
    "The first point of the theorem states that the solutions of the optimality equations are the optimal value functions from period $t$ onward. The second point means that the equation for $t=0$ gives the optimal value of the MDP. The third point shows how to construct an optimal policy from the solutions of the optimality equations. This policy is obtained by choosing at each decision epoch an action that attains the maximum in the optimality equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5886c166c321c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Uniqueness of Optimal Policies\n",
    "\n",
    "1. What can be said on the set of optimal policies? Is it a singleton?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbfd52db96ebd3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "##### Answer:\n",
    "No, the set of optimal policies is not necessarily a singleton.\n",
    "Indeed, it is enough that there exist multiple actions that achieve the maximum in the optimality equations for some state $s \\in \\mathcal{S}$ at some time $t$ to have multiple optimal policies.\n",
    "In such cases, any policy that selects one of these maximizing actions at each decision epoch will be optimal.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0822ce",
   "metadata": {},
   "source": [
    "### Optimal Policies for Finite-Horizon MDPs are Deterministic and Markovian\n",
    "\n",
    "**Theorem:**\n",
    "Let $u_t^*$, $t=0, 1, \\dots, N$ be a collection of functions satisfying the optimality equations.\n",
    "Then,\n",
    "1.  For each $t = 0, 1, \\dots, N-1$, $u_t^*(h_t)$ depends only on the current state $s_t$.\n",
    "2.  If the maximum in the optimality equation is attained by some action $a \\in \\mathcal{A}$ for each $t$ and each history $h_t$, then there exists a *deterministic, Markovian optimal policy* $\\pi^* = (d_0^*, d_1^*, \\dots, d_{N-1}^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514018a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Optimality Equations for Markovian Policies\n",
    "\n",
    "Give the optimality equations when the policy is restricted to be Markovian.\n",
    "Give the expression of the optimal policy in this case.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d9294",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "For any $t = N-1, N-2, \\dots, 0$ and for all states $s_t \\in \\mathcal{S}$, the optimality equations for Markovian policies are given by:\n",
    "\n",
    "- $u_t^*(s_t) = \\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}^*(s') \\right\\}$\n",
    "with the terminal condition:\n",
    "- $u_N^*(s_N) = r_N(s_N)$\n",
    "\n",
    "The optimal policy in this case is given by:\n",
    "\n",
    "- $d_t^*(s_t) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}^*(s') \\right\\}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761e4c7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Computational Complexity of the Optimality Equations for Markov Policies\n",
    "\n",
    "How many multiplications are required to compute $v^*_N(s)$ for all $s \\in \\mathcal{S}$ using the optimality equations for Markov policies?\n",
    "Express your answer in terms of $N$ (the horizon length), $|\\mathcal{S}|$ (the number of states), and $|\\mathcal{A}|$ (the number of actions).\n",
    "\n",
    "How many multiplications would be required if a direct evaluation of all possible non-stationary Markovian policies were used instead?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b95659",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "The total number of multiplications required to compute $v^*_N(s)$ for all $s \\in \\mathcal{S}$ using the optimality equations is given by:\n",
    "\n",
    "$ N \\times |\\mathcal{S}|^2 \\times |\\mathcal{A}| $\n",
    "\n",
    "This is because for each of the $N$ decision steps, we compute the value for each of the $|\\mathcal{S}|$ states, and for each state, we evaluate $|\\mathcal{A}|$ actions, each involving a sum over $|\\mathcal{S}|$ next states.\n",
    "\n",
    "\n",
    "If a direct evaluation of all possible non-stationary Markovian policies were used instead, the number of multiplications required would be:\n",
    "\n",
    "$|\\mathcal{A}|^{|\\mathcal{S}| \\times N} \\times N \\times |\\mathcal{S}|^2 $\n",
    "\n",
    "This is because there are $|\\mathcal{A}|^{|\\mathcal{S}| \\times N}$ possible policies, and for each policy, we would need to perform policy evaluation, which requires $N \\times |\\mathcal{S}|^2$ multiplications.\n",
    "\n",
    "For $|\\mathcal{S}| = 2$, $|\\mathcal{A}| = 2$, and $N = 5$, this results in:\n",
    "- Using optimality equations: $5 \\times 2^2 \\times 2 = 40$ multiplications.\n",
    "- Using direct evaluation: $2^{2 \\times 5} \\times 5 \\times 2^2 = 20480$ multiplications.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea168203",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Solve the Stochastic Inventory Control Problem\n",
    "\n",
    "Consider the stochastic inventory control problem defined above with an horizon of $N = 3$ periods, maximum inventory capacity of $M = 3$ units, and the following parameters:\n",
    "\n",
    "- Demand probabilities: $p_0 = \\frac{1}{4}$, $p_1 = \\frac{1}{2}$, $p_2 = \\frac{1}{4}$, and $p_j = 0$ for $j \\geq 3$\n",
    " (It is supposed that the demand is never greater than 2, almost surely)\n",
    "\n",
    "\n",
    "- Ordering cost: $O(s) = 4 + 2s$\n",
    "- Holding cost: $h(s) = s$\n",
    "- Terminal inventory value: $g(s) = 0$\n",
    "\n",
    "- Revenue function: $F$ is defined by $F(0) = 0$, $F(1) = 6$, $F(2) = F(3) = 8$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Calculate the transition probabilities.\n",
    "\n",
    "Calculate the reward function.\n",
    "\n",
    "Apply the optimality backward equations to find the optimal value function and an optimal policy (you can write a program to facilitate this).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50996939",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "\n",
    "**Transition Probabilities:**\n",
    "The transition probabilities $p_t(s'|s, a)$ are dependent on the inventory level after ordering, which is $s+a$.\n",
    "This means two states $s$ and $s'$ and actions $a$ and $a'$ lead to the same transition probabilities if $s+a = s'+a'$.\n",
    "The table below shows the probability of transitioning to a new inventory level $s'$ (columns) based on the inventory level after ordering (rows).\n",
    "\n",
    "| Inventory after order ($s+a$) | Next state $s'=0$ | Next state $s'=1$ | Next state $s'=2$ | Next state $s'=3$ |\n",
    "|:-----------------------------:|:----------------:|:----------------:|:----------------:|:----------------:|\n",
    "| **0**                         | 1                | 0                | 0                | 0                |\n",
    "| **1**                         | 3/4              | 1/4              | 0                | 0                |\n",
    "| **2**                         | 1/4              | 1/2              | 1/4              | 0                |\n",
    "| **3**                         | 0                | 1/4              | 1/2              | 1/4              |\n",
    "\n",
    "*Note: The last row for s+a=3 assumes demand can be at most 2. If demand could be 3, the last probability for j=3 would be 1/4, and for j=0 it would be 1/4.*\n",
    "\n",
    "**Reward Function:**\n",
    "The reward $r_t(s, a)$ is calculated for each state $s$ (rows) and action $a$ (columns). The `x` indicates an invalid action (e.g., ordering more than the capacity allows).\n",
    "\n",
    "| State ($s$) \\ Action ($a$) | 0    | 1    | 2    | 3    |\n",
    "|:--------------------------:|:----:|:----:|:----:|:----:|\n",
    "| **0**                      | 0    | -1   | -2   | -5   |\n",
    "| **1**                      | 5    | 0    | -3   | x    |\n",
    "| **2**                      | 6    | -1   | x    | x    |\n",
    "| **3**                      | 5    | x    | x    | x    |\n",
    "\n",
    "**Optimality Backward Equations:**\n",
    "\n",
    "In the following, let $u_t^*(s, a) = r_t(s, a) + \\sum_{s'} p_t(s' | s, a) u_{t+1}^*(s')$.\n",
    "\n",
    "\n",
    "1. **At $t=N=3$**:\n",
    "   - $u_3^*(s) = r_3(s) = 0$ for all $s \\in \\{0, 1, 2, 3\\}$.\n",
    "\n",
    "\n",
    "2. **At $t=2$**:\n",
    "    - Calculate $u_2^*(s)$ for each state $s$ using the optimality equations.\n",
    "    \n",
    "    $u_2^*(s) = \\max_{a} \\left\\{ r_2(s, a) + \\sum_{s'} p_2(s' | s, a) u_3^*(s') \\right\\} = \\max_{a} r_2(s, a)$ since $u_3^*(s') = 0$.\n",
    "\n",
    "Inspecting the reward table shows that in each state, the maximum reward is obtained by choosing action $a=0$, that is, not ordering any new stock.\n",
    "Thus,\n",
    "\n",
    "| State ($s$) | $u_2^*(s)$ | Optimal Action ($a^*$) |\n",
    "|:-----------:|:----------:|:-----------------------:|\n",
    "| **0**       | 0          | 0                       |\n",
    "| **1**       | 5          | 0                       |\n",
    "| **2**       | 6          | 0                       |\n",
    "| **3**       | 5          | 0                       |\n",
    "\n",
    "3. **At $t=1$**:\n",
    "    - Calculate $u_1^*(s)$ for each state $s$ using the optimality equations.\n",
    "\n",
    "    Compute $u_1^*(s, a)$ for each state and action:\n",
    "    \n",
    "    $u_1^*(s, a) = r_1(s, a) + \\sum_{s'} p_1(s' | s, a) u_2^*(s')$.\n",
    "\n",
    "For instance,\n",
    "\n",
    " $u_1^*(0, 2) = r_1(0, 2) + p_1(0|0,2)u_2^*(0) + p_1(1|0,2)u_2^*(1) + p_1(2|0,2)u_2^*(2) + p_1(3|0,2)u_2^*(3) = -2 + (1/4)*0 + (1/2)*5 + (1/4)*6 + 0 = -2 + 0 + 2.5 + 1.5 = 2$.\n",
    "\n",
    "\n",
    "The quantities $u_1^*(s, a)$, $u_1^*(s)$, and the optimal decisions are summarized in the following table.\n",
    "\n",
    "| $s$ | $a=0$ | $a=1$ | $a=2$ | $a=3$ | $u_1^*(s)$ | $a^*$ |\n",
    "|:---:|:-----:|:-----:|:-----:|:-----:|:----------:|:-----------:|\n",
    "| 0   | 0     | 1/4   | 2     | 1/2   | 2          | 2           |\n",
    "| 1   | 25/4  | 4     | 5/2   | x     | 25/4       | 0           |\n",
    "| 2   | 10    | 9/2   | x     | x     | 10         | 0           |\n",
    "| 3   | 21/2  | x     | x     | x     | 21/2       | 0           |\n",
    "\n",
    "3. **At $t=0$**:\n",
    "    - Calculate $u_0^*(s)$ for each state $s$ using the optimality equations.\n",
    "    $u_0^*(s) = \\max_{a \\in A_s} \\{u_1^*(s,a)\\}$.\n",
    "    The quantities $u_0^*(s, a)$, $u_0^*(s)$ are summarized in the following table.\n",
    "\n",
    "| $s$ | $a=0$ | $a=0$ | $a=2$ | $a=3$ | $u_0^*(s)$ | $a^*$ |\n",
    "|:---:|:-----:|:-----:|:-----:|:-----:|:----------:|:-----------:|\n",
    "| 0   | 2     | 33/16 | 66/16 | 67/16 | 67/16      | 3           |\n",
    "| 1   | 129/16| 98/16 | 99/16 | x     | 129/16     | 0           |\n",
    "| 2   | 194/16| 131/16| x     | x     | 194/16     | 0           |\n",
    "| 3   | 227/16| x     | x     | x     | 227/16     | 0           |\n",
    "\n",
    "\n",
    "This algorithm yields the optimal expected total rewards function $v^*_N(s) = u_0^*(s)$ and the optimal policy $\\pi^* = (d_0^*, d_1^*, d_2^*)$ where:\n",
    "- $d_0^*(0) = 3$, $d_0^*(1) = 0$, $d_0^*(2) = 0$, $d_0^*(3) = 0$\n",
    "- $d_1^*(0) = 2$, $d_1^*(1) = 0$, $d_1^*(2) = 0$, $d_1^*(3) = 0$\n",
    "- $d_2^*(0) = 0$, $d_2^*(1) = 0$, $d_2^*(2) = 0$, $d_2^*(3) = 0$\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m2ai_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
