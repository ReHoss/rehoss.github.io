{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a37a6c9860e28d",
   "metadata": {},
   "source": [
    "# Practical Session 1: Markov Decision Processes and Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c08d0b",
   "metadata": {},
   "source": [
    "##### *M2 Artificial Intelligence (Paris Saclay University) - Reinforcement Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68d5df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9bb174ec7f892",
   "metadata": {},
   "source": [
    "From Bellman's preface to \"Dynamic Programming\" (1957):\n",
    "\n",
    "*The purpose of this work is to provide an introduction to the mathematical theory of multi-stage decision processes. Since these constitute a somewhat formidable set of terms we have coined the term “dynamic programming” to describe the subject matter.*\n",
    "\n",
    "*The problems we treat are programming problems, to use a terminology now popular. The adjective “dynamic,” however, indicates that we are interested in processes in which time plays a significant role, and in which the order of operations may be crucial. However, an essential feature of our approach will be the reinterpretation of many static processes as dynamic processes in which time can be artificially introduced.*\n",
    "\n",
    "The term \"programming\" here does not refer to computer programming, but rather to the process of planning or decision-making. See this [Wikipedia section](https://en.wikipedia.org/wiki/Dynamic_programming#History_of_the_name).\n",
    "\n",
    "This practical session is based on the chapter 4 of the book \"Markov Decision Processes: Discrete Stochastic Dynamic Programming\" by Martin L. Puterman (1994)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3d3dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f298967bcca51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:48.944241Z",
     "start_time": "2025-11-09T19:16:48.939693Z"
    }
   },
   "source": [
    "## Part I: Modelling a Decision Problem as a Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d1c4a45bd6c9c",
   "metadata": {},
   "source": [
    "The most fundamental sequential decision-making model is the **Markov Decision Process (MDP)**.\n",
    "\n",
    "### Definition: Markov Decision Process\n",
    "\n",
    "A Markov Decision Process provides a formal way to describe an environment for decision-making under uncertainty. A (finite) MDP is defined by a 4-tuple $(\\mathcal{S}, \\mathcal{A}, P, r)$:\n",
    "\n",
    "* **$\\mathcal{S}$**: A finite set of **states** $s \\in \\mathcal{S}$.\n",
    "* **$\\mathcal{A}$**: A finite set of **actions** $a \\in \\mathcal{A}$. We often write $\\mathcal{A}(s)$ to denote the set of actions available in state $s$.\n",
    "* **$P$**: The **transition probability function**, $P(s' | s, a) = \\mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]$. This defines the \"dynamics\" of the environment.\n",
    "* **$r$**: The **reward function**, $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. This gives the immediate reward $r(s, a)$ received after taking action $a$ in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c05b7",
   "metadata": {},
   "source": [
    "### Example: The Two-State Problem\n",
    "\n",
    "The system can be in one of two states, $s_1$ or $s_2$.\n",
    "\n",
    "* In state **$s_1$**, the decision-maker can choose $a_{1,1}$ or $a_{1,2}$.\n",
    "* In state **$s_2$**, only action $a_{2,1}$ is available.\n",
    "\n",
    "The problem dynamics and rewards are summarized in the following table:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\text{State} & \\text{Action} & \\text{Next State(s)} & \\text{Reward} \\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\large s_1 & a_{1,1} & \\begin{cases} s_1 & (p=0.5) \\\\ s_2 & (p=0.5) \\end{cases} & 5 \\\\\n",
    "\\hline\n",
    "\\large s_1 & a_{1,2} & s_2 \\quad (p=1.0) & 10 \\\\\n",
    "\\hline\n",
    "\\large s_2 & a_{2,1} & s_2 \\quad (p=1.0) & -1 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b885a480635b105",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Define the MDP for the Two-State Problem\n",
    "\n",
    "Define the components of the MDP for the two-state problem:\n",
    "\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):**\n",
    "\n",
    "* **Action Space ($\\mathcal{A}(s)$):**\n",
    "\n",
    "* **Reward Function ($r(s, a)$):**\n",
    "\n",
    "* **Transition Probabilities ($P(s' | s, a)$):**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d617c",
   "metadata": {},
   "source": [
    "### Example: Single Product Stochastic Inventory Control\n",
    "\n",
    "\n",
    "#### Description:\n",
    "Inventory models were among the first problems solved using Markov decision problem methods, and their study has motivated many theoretical developments.\n",
    "\n",
    "Each month, the manager of a warehouse determines current inventory (stock on hand) of a single product. Based on this information, he decides whether or not to order additional stock from a supplier.\n",
    "In doing so, he is faced with a tradeoff between the costs associated with keeping inventory and the lost sales or penalties associated with being unable to satisfy customer demand for the product.\n",
    "The manager's objective is to maximize some measure of profit (sales revenue less inventory holding and ordering costs) over the decision-making horizon.\n",
    "Demand for the product is random with a known probability distribution.\n",
    "\n",
    "We formulate a model under the following set of simplifying assumptions. Generalizations which make the model more realistic are explored through problems at the end of the chapter.\n",
    "1. The decision to order additional stock is made at the beginning of each month\n",
    "and delivery occurs instantaneously.\n",
    "2. Demand for the product arrives throughout the month but all orders are filled\n",
    "on the last day of the month.\n",
    "3. If demand exceeds inventory, the customer goes elsewhere to purchase\n",
    "product; that is, there is no backlogging of unfilled orders so that excess\n",
    "demand is lost.\n",
    "4. The revenues, costs, and the demand distribution do not vary from month to\n",
    "month.\n",
    "5. The product is sold only in whole units.\n",
    "6. The warchouse has capacity of $M$ units.\n",
    "\n",
    "Let $s_t$ denote the inventory on hand at the beginning of month $t$, $a_t$ the number of units ordered by the inventory manager in month $t$ and $D_t$ the random demand in month $t$. We assume that the demand has a known time-homogeneous probability distribution $p_j = P(D_t = j)$, $j \\in \\mathbb{N}$.\n",
    "The inventory at decision epoch $t + 1$, $s_{t+1}$, is related to the inventory at decision epoch $t$, $s_t$, through the system equation\n",
    "$$ s_{t+1} = \\max\\{s_t + a_t - D_t, 0\\} = [s_t + a_t - D_t]^{+}. $$\n",
    "Because backlogging is not permitted, the inventory level cannot be negative. Thus whenever $s_t + a_t - D_t < 0$, the inventory level at the subsequent decision epoch is 0.\n",
    "We now describe the economic parameters of this model. We express them as values at the start of the month so that we are implicitly considering the time value of money when defining these quantities. We refer to them as present values to\n",
    "\n",
    "A Markov decision process formulation follows.\n",
    "\n",
    "\n",
    "**Expected rewards (expected revenue less ordering and holding costs):**\n",
    "The reward is composed of three parts: the expected revenue from sales, the cost of ordering new stock, and the cost of holding inventory.\n",
    "\n",
    "The one-step expected reward is:\n",
    "$r_t(s, a) = F(s+a) - O(a) - h(s+a), \\quad t=1, 2, \\dots, N-1$\n",
    "\n",
    "\n",
    "- The expected revenue $F(s)$ when having $s$ units of inventory.\n",
    "- $O(a)$ is the cost of ordering $a$ units.\n",
    "- $h(s+a)$ is the cost of holding $s+a$ units of inventory for a month.\n",
    "\n",
    "\n",
    "At the end of the horizon, there is a terminal reward:\n",
    "$r_N(s) = g(s), \\quad t=N$.\n",
    "where $g(s)$ is the value of the terminal inventory.\n",
    "\n",
    "**Transition probabilities:**\n",
    "If the inventory on hand at the beginning of period $t$ is $s$ units and an order is placed for $a$ units, the inventory prior to external demand is $s+a$ units.\n",
    "An inventory level of $s'>0$ at the start of period $t+1$ requires a demand of $s+a-s'$ units in period $t$. This occurs with probability $p_{s+a-s'}$.\n",
    "Because backlogging is not permitted, if the demand in period $t$ exceeds $s+a$ units, then the inventory at the start of period $t+1$ is 0 units.\n",
    "This occurs with probability $\\sum_{k=s+a}^{\\infty} p_k = P(D_t \\ge s+a)$.\n",
    "The probability that the inventory level exceeds $s+a$ units is 0, since demand is non-negative.\n",
    "Assumption 6 constrains the inventory always to be less than or equal to $M$.\n",
    "\n",
    "Consequently,\n",
    "$P(s'|s, a) = \\begin{cases} 0 & \\text{if } M \\ge s' > s+a \\\\ p_{s+a-s'} & \\text{if } M \\ge s+a \\ge s' > 0 \\\\ \\sum_{k=s+a}^{\\infty} p_k & \\text{if } M \\ge s+a \\text{ and } s'=0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b609f391f5b610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Define the MDP for the Inventory Control Problem\n",
    "\n",
    "Define the missing components of the MDP for the inventory control problem:\n",
    "\n",
    "\n",
    "* **State Space ($\\mathcal{S}$):**\n",
    "\n",
    "* **Action Space ($\\mathcal{A}(s)$):**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc5bed",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d01bd",
   "metadata": {},
   "source": [
    "## Part II: Policy and Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba60171",
   "metadata": {},
   "source": [
    "### Definition (Decision Rule and Policy):\n",
    "\n",
    "A **decision rule** $d_t$ at time $t \\in \\{0, 1, \\dots, N-1\\}$ is a function that maps a history of states and actions $H_t$ to a probability distribution over the set of actions $\\mathcal{A}$.\n",
    "$d_t: H_t \\to \\mathcal{P}(\\mathcal{A})$\n",
    "where $H_t = \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times \\dots \\times \\mathcal{A} \\times \\mathcal{S}$ is the set of all possible histories up to time $t$.\n",
    "When the decision rule is deterministic, it maps histories directly to actions:\n",
    "$d_t: H_t \\to \\mathcal{A}$.\n",
    "\n",
    "\n",
    "A **policy** $\\pi$ is a sequence of decision rules for each time step.\n",
    "$\\pi = (d_0, d_1, \\dots, d_{N-1})$\n",
    "\n",
    "A policy is **Markovian** if the decision rules only depend on the current state $s_t$, not the entire history.\n",
    "A policy is **stationary** if the decision rule is the same for all time steps (i.e., $d_t = d$ for all $t$, then $\\pi$ can be represented by a single decision rule $d$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c3300",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Defining Policies\n",
    "\n",
    "Suppose N = 2.\n",
    "\n",
    "Define a deterministic Markovian policy $\\pi = (d_0, d_1)$ and a stochastic Markovian policy $\\pi = (d_0, d_1)$ for the two-state problem defined earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc61282",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: On the types of Policies\n",
    "\n",
    "In the two-state problem defined earlier, what can be said about the set of history-dependent policies compared to the set of Markovian policies?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b9b0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Cardinality of Deterministic, Markovian Policies\n",
    "What is the size of the set of deterministic, non-stationary Markovian policies for an MDP with $|\\mathcal{S}|$ states and $|\\mathcal{A}|$ actions?\n",
    "\n",
    "What is the size of the set of deterministic, stationary Markovian policies for the same MDP?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05401d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: \n",
    "\n",
    "Modify the previous model by adding an additional action $a_{1,3}$ in state $s_1$ that leads back to state $s_1$ with probability 1, *i.e.* $P(s_1 | s_1, a_{1,3}) = 1$ with reward $r(s_1, a_{1,3}) = 0$. Note that $\\mathcal{A}(s_1) = \\{a_{1,1}, a_{1,2}, a_{1,3}\\}$ now.\n",
    "\n",
    "Give an history-dependent deterministic policy.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819edf0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Comparing Policies\n",
    "\n",
    "Let $N=2$ and let $R_t \\coloneqq r(S_t, A_t)$ be the reward received at time $t$.\n",
    "\n",
    "Suppose there are two policies $\\pi$ and $\\pi'$ such that:\n",
    "\n",
    "$P^{\\pi}[R_0 = 0, R_1 = 0] = P^{\\pi}[R_0 = 0, R_1 = 1] = P^{\\pi}[R_0 = 1, R_1 = 0] = P^{\\pi}[R_0 = 1, R_1 = 1] = \\frac{1}{4}$\n",
    "\n",
    "and\n",
    "\n",
    "$P^{\\pi'}[R_0 = 0, R_1 = 0] = P^{\\pi'}[R_0 = 1, R_1 = 1] = \\frac{1}{2}$\n",
    "\n",
    "Compare these policies on the basis of their expected total rewards: \n",
    "\n",
    "$E^\\pi[R_1 + R_2] = E^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)]$.\n",
    "\n",
    "Compare the variance of the total rewards under each policy:\n",
    "\n",
    "$Var^\\pi[R_1 + R_2] = Var^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)] = E^\\pi [(\\sum_{t=0}^{N-1} r(S_t, A_t) - E^\\pi[\\sum_{t=0}^{N-1} r(S_t, A_t)])^2]$\n",
    "\n",
    "Which policy would you prefer and why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86613bc922d5e43",
   "metadata": {},
   "source": [
    "### Definition (The Expected Total Reward Criterion):\n",
    "Let $v^\\pi_{N}(s)$ represent the expected total reward over the decision making horizon if policy $\\pi$ is followed, starting from state $s$ at the first decision epoch (time $t=0$):\n",
    "\n",
    "$v^\\pi_{N}(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{N-1} r(S_t, A_t) + r_N(S_N) \\mid S_0 = s \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ece05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Expected Reward for a Markovian Policy\n",
    "Suppose $\\pi$ is a deterministic, Markovian policy. Simplify the expression for the expected total reward $v^\\pi_N(s)$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9f7ac15df814c",
   "metadata": {},
   "source": [
    "### Finite-Horizon Policy Evaluation\n",
    "\n",
    "Markov decision problem theory and computation is based on using **backward induction** (dynamic programming) to recursively evaluate expected rewards. This section introduces the fundamental recursion of dynamic programming by providing an efficient method to evaluate the expected total reward of a fixed policy.\n",
    "\n",
    "Let $\\pi = (d_1, d_2, \\dots, d_{N-1})$ be a policy. Let $u_t^\\pi: H_t \\to \\mathbb{R}$ denote the total expected reward obtained by using policy $\\pi$ at decision epochs $t, t+1, \\dots, N-1$. If the history at decision epoch $t$ is $h_t \\in H_t$, then for $t < N$:\n",
    "\n",
    "$u_t^\\pi(h_t) = E^\\pi [ \\sum_{n=t}^{N-1} r_n(X_n, Y_n) + r_N(X_N)  | H_t = h_t ]$\n",
    "\n",
    "We can compute this value by an inductive evaluation. To simplify the notation, we assume that we are using a deterministic policy $\\pi$.\n",
    "\n",
    "#### The Finite Horizon-Policy Evaluation Algorithm (for a fixed deterministic policy $\\pi$)\n",
    "\n",
    "1.  **Set** $t=N$ and initialize the terminal reward for all possible final histories $h_N = (h_{N-1}, a_{N-1}, s_N)$:\n",
    "    $u_N^\\pi(h_N) = r_N(s_N)$\n",
    "\n",
    "2.  **If** $t=0$, stop. Otherwise, go to step 3.\n",
    "\n",
    "3.  **Substitute** $t-1$ for $t$ and compute $u_t^\\pi(h_t)$ for each history $h_t = (h_{t-1}, a_{t-1}, s_t)$ using the recursive formula:\n",
    "    $u_t^\\pi(h_t) = r_t(s_t, d_t(h_t)) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, d_t(h_t)) u_{t+1}^\\pi(h_t, d_t(h_t), s')$\n",
    "\n",
    "4.  **Return** to step 2.\n",
    "\n",
    "This inductive scheme reduces the problem of computing expected total rewards over $N + 1$ periods to a sequence of $N$ similar one-period calculations. The expected value of the policy $\\pi$ over the periods from $t$ to $N$ given the history at epoch $t$ is the sum of the immediate reward from taking action $d_t(h_t)$ and the expected total reward over the remaining periods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c04cd5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Computational Complexity of Policy Evaluation\n",
    "\n",
    "How many multiplications are required to compute $v^\\pi_N(s)$ for all $s \\in \\mathcal{S}$ using the finite-horizon policy evaluation algorithm? Express your answer in terms of $N$ (the horizon length), $|\\mathcal{S}|$ (the number of states), and $|\\mathcal{A}|$ (the number of actions).\n",
    "Give also the computational complexity when the policy $\\pi$ is Markovian.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf357f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Policy Evaluation Expressions\n",
    "\n",
    "1. Give the expression for $u_t^\\pi(h_t)$ when the policy $\\pi$ is Markovian.\n",
    "2. Give the expression for $u_t^\\pi(h_t)$ in terms of conditional expectations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953af096",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51836b",
   "metadata": {},
   "source": [
    "## Part III: Optimality Equations and Bellman's Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5fb2a",
   "metadata": {},
   "source": [
    "### Definition (Optimal Policy):\n",
    "An **optimal policy** $\\pi^*$ is a policy that maximizes the expected total reward for every starting state $s$.\n",
    "$v^{\\pi^*}_N(s) \\ge v^\\pi_N(s), \\quad \\text{for all } s \\in \\mathcal{S} \\text{ and for all } \\pi$\n",
    "\n",
    "We seek to characterize the value of the Markov decision problem, $v^*_N(s)$, defined by\n",
    "$v^*_N(s) = \\sup_{\\pi} v^\\pi_N(s), \\quad s \\in \\mathcal{S}$\n",
    "and when the supremum is attained, by\n",
    "$v^*_N(s) = \\max_{\\pi} v^\\pi_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "The expected total reward of an optimal policy $\\pi^*$ satisfies\n",
    "$v^{\\pi^*}_N(s) = v^*_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68215419a6d28709",
   "metadata": {},
   "source": [
    "### Optimality Equations (Bellman Equations) and The Principle of Optimality\n",
    "\n",
    "Let $u_t: H_t \\to \\mathbb{R}$ for $t = 0, 1, \\dots, N$ be a collection of functions defined on the set of histories at each decision epoch.\n",
    "The *optimality equations* for the collection of functions $(u_t)_{t=0}^N$ are given for any $t = N-1, N-2, \\dots, 0$ and for all histories $h_t \\in H_t$ by:\n",
    "\n",
    "- $u_t(h_t) = \\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}(h_t, a, s') \\right\\}$\n",
    "\n",
    "with the terminal condition:\n",
    "\n",
    "- $u_N(h_N) = r_N(s_N)$\n",
    "\n",
    "--\n",
    "\n",
    "**Theorem (Principle of Optimality):**\n",
    "Suppose that the collection of functions $(u_t^*)_{t=0}^N$ satisfies the optimality equations.\n",
    "Then, for each $t = 0, 1, \\dots, N-1$ and for all histories $h_t \\in H_t$, suppose that a policy\n",
    "\n",
    "- $u_t^*(h_t) = \\max_{\\pi} u_t^\\pi(h_t) = \\max_{\\pi} E^\\pi \\left[ \\sum_{n=t}^{N-1} r_n(X_n, Y_n) + r_N(X_N) | H_t = h_t \\right]$\n",
    "\n",
    "- $u_0^*(s) = v^*_N(s), \\quad s \\in \\mathcal{S}$\n",
    "\n",
    "Moreover, a policy $\\pi^* = (d_0^*, d_1^*, \\dots, d_{N-1}^*)$ defined by\n",
    "\n",
    "- $d_t^*(h_t) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ r_t(s_t, a) + \\sum_{s' \\in \\mathcal{S}} p_t(s' | s_t, a) u_{t+1}^*(h_t, a, s') \\right\\}$\n",
    "\n",
    "is an optimal policy, i.e., $v^{\\pi^*}_N(s) \\geq v^\\pi_N(s)$ for all $s \\in \\mathcal{S}$ and for all policies $\\pi$ (and $v^{\\pi^*}_N(s) = v^*_N(s)$ for all $s \\in \\mathcal{S}$).\n",
    "\n",
    "--\n",
    "\n",
    "The first point of the theorem states that the solutions of the optimality equations are the optimal value functions from period $t$ onward. The second point means that the equation for $t=0$ gives the optimal value of the MDP. The third point shows how to construct an optimal policy from the solutions of the optimality equations. This policy is obtained by choosing at each decision epoch an action that attains the maximum in the optimality equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5886c166c321c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Uniqueness of Optimal Policies\n",
    "\n",
    "1. What can be said on the set of optimal policies? Is it a singleton?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0822ce",
   "metadata": {},
   "source": [
    "### Optimal Policies for Finite-Horizon MDPs are Deterministic and Markovian\n",
    "\n",
    "**Theorem:**\n",
    "Let $u_t^*$, $t=0, 1, \\dots, N$ be a collection of functions satisfying the optimality equations.\n",
    "Then,\n",
    "1.  For each $t = 0, 1, \\dots, N-1$, $u_t^*(h_t)$ depends only on the current state $s_t$.\n",
    "2.  If the maximum in the optimality equation is attained by some action $a \\in \\mathcal{A}$ for each $t$ and each history $h_t$, then there exists a *deterministic, Markovian optimal policy* $\\pi^* = (d_0^*, d_1^*, \\dots, d_{N-1}^*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514018a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Optimality Equations for Markovian Policies\n",
    "\n",
    "Give the optimality equations when the policy is restricted to be Markovian.\n",
    "Give the expression of the optimal policy in this case.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761e4c7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Computational Complexity of the Optimality Equations for Markov Policies\n",
    "\n",
    "How many multiplications are required to compute $v^*_N(s)$ for all $s \\in \\mathcal{S}$ using the optimality equations for Markov policies?\n",
    "Express your answer in terms of $N$ (the horizon length), $|\\mathcal{S}|$ (the number of states), and $|\\mathcal{A}|$ (the number of actions).\n",
    "\n",
    "How many multiplications would be required if a direct evaluation of all possible non-stationary Markovian policies were used instead?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea168203",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Question: Solve the Stochastic Inventory Control Problem\n",
    "\n",
    "Consider the stochastic inventory control problem defined above with an horizon of $N = 3$ periods, maximum inventory capacity of $M = 3$ units, and the following parameters:\n",
    "\n",
    "- Demand probabilities: $p_0 = \\frac{1}{4}$, $p_1 = \\frac{1}{2}$, $p_2 = \\frac{1}{4}$, and $p_j = 0$ for $j \\geq 3$\n",
    " (It is supposed that the demand is never greater than 2, almost surely)\n",
    "\n",
    "\n",
    "- Ordering cost: $O(s) = 4 + 2s$\n",
    "- Holding cost: $h(s) = s$\n",
    "- Terminal inventory value: $g(s) = 0$\n",
    "\n",
    "- Revenue function: $F$ is defined by $F(0) = 0$, $F(1) = 6$, $F(2) = F(3) = 8$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Calculate the transition probabilities.\n",
    "\n",
    "Calculate the reward function.\n",
    "\n",
    "Apply the optimality backward equations to find the optimal value function and an optimal policy (you can write a program to facilitate this).\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m2ai_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
